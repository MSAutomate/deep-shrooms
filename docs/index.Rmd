---
title: 'Deep Shrooms: classifying mushroom images'
author: 'Teemu Koivisto, Tuomo Nieminen, Jonas Harjunp채채'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

Deep Shrooms is a project by three students from the University of Helsinki (HY): Teemu Koivisto, Tuomo Nieminen, Jonas Harjunp채채. The project relates to the HY course module 'Introduction to Data Science' (fall 2017). The goal of the project is to develop a smartphone application to classify wild mushrooms to poisonous/edible based on image input from the users camera.  

The project uses tools of data wrangling, machine learning and [TODO: insert smartphone app techonology term]. The project naturally splits into three parts: (1) retrieving and transforming data, (2) training a classifier, (3) building an application.

In this blog post, we'll describe our project and the app building process. We'll showcase our classifiers correctness using summary statistics that communicate the accuracy and predictive power of our model. We'll pay close attention to possible false edibles: we do not wish users of our app to end up eating poisonous mushrooms. We hope that the app would be a fun tool for the curious mind.

## Technical introduction

The specific goal of our project is to create a smartphone application which classifies user inputted pictures of mushrooms to poisonous/edible class using a machine learning classifier such as a convolutional neural network (CNN). CNN:s are the current state-of-the-art in image recognition. 

To train such a classifier, we'll need large amounts of pictures of previously classified mushrooms to use as examples. We cannot simply use any pictures of mushrooms, since they have to at least include a reliable poisonous/edible tag. We'll also have to normalize the size and shape of any pictures we'll find, since usually classification algorithms expect prespecified input sizes.

As already mentioned in the introduction, we'll pay close attention to the possibility of false edibles. Instead of simply outputting a binary poisonous or edible status, we'll instead provide the user with the *probability* of having a poisonous mushroom in their hands. Therefore the user can make a choice to only eat mushrooms with very low probability of poison.

## Domain introduction

Humans are very good at categorizing items in images. Based on previous expert knowledge, the following features would be very useful to extract for mushroom classification:

1) Note where the mushroom grows: is it on the ground or on wood.  
1.1) If it grows on ground it could be a saprotroph of detritus (karikkeen lahottaja), mycorrhiza (juurisieni) which is specific type of mushroom living from the roots of a tree or it could still be a saprotroph of a tree but the wood is on ground level.  
1.2) If it grows on wood consider if it's conifer (havupuu) or hardwood (lehtipuu). Some species grow on only a very specific wood like oak.  
2) Do not consider the time of year to be a indicator of any sort. Every year the seasons length differ so one year the mushroom season might start way later than the next year.  
3) Underneath the mushroom cap can be different types of gills in various combinations.  
4) Surface of the cap is also a very good classifier of the mushroom. The structure might not be possible to know from a picture though.  
5) Color of the mushroom varies a lot depending on the humidity and the age of the mushroom. When raining the colors get deeper and more distinctive. Young mushrooms have stronger colors than old ones. Also sunlight might diminish the colors.  
6) Stipe (*jalka* in Finnish) of a mushroom can also indicate a lot of information from the mushroom. Thin and thick shapes are more distinctive than average size.  
7) Geographic location of the mushroom could be useful as some species probably don't grow everywhere in Finland.

There is also a lot of data such as smell and touch that could be useful. However, we do not expect to be able to explicitly extract many of these features. However, a CNN can perform feature extraction automatically. It is "simply" a matter of defining the structure of the network in such a way that the network is able to extract the information it needs for the classification task.

# Retrieving and transforming data

www.mushroom.world contains a large database of mushroom pictures and information such as edibility status. We build a web scraped which retrieves images and their information from the website. From these mushrooms we have collected besides the edibility their names in Finnish, English and Latin. We also collected the URL:s for the shroom and the pictures available on the site for each mushroom, from which we then downloaded the images. Since the edibility was originally classified into 7 different classes, like "edible and good", "leathally poisonous" and "edible and excellent", we decided to narrow the edibility down to edible or non-edible. We categorized "edible when cooked" as non-edible, which means that all edible mushrooms require no preprocessing and should be safe to eat. 

The images and the metadata are stored in both Google Drive and Amazon S3. The original pictures were very good quality and therefore took a lot of space. We used .jpg compression and resized the images. In the original set of images, the sizes varied quite a lot. To solve this we standardized the pictures to the size of 480x480, by padding the missing size with the edge values. In order for the model to work, we needed to scale the channel from the original scale of 0-255 to a scale between 0-1.

Our images are represented as tuples of size 
(480, 480, 3) where the dimension correspond to "width", "height" and "channel". The last 
dimension is the rgb channel, meaning that the image can be thought of as three matrices stacked over each other, 
where each matrix represents the amount of red green or blue in the picture. 


# Using augmentation to artificially increase the amount of data

Our mushroom dataset contains some hunderds of pictures. It is however well known that  deep learning methods start to work well when them amount of data quite a lot more, preferably in the tens of thousands if not millions of examples. We were well off the mark.

We decided to "cheat" by artificially increasing the number of unique training examples by introducing small distortions to the images. We used the **keras** deep learning library to build an image generator which, instead of the original images, produces an infinite stream of randomly altered images.

```{r, out.width = "600px", out.height = "600px", fig.cap = "Examples of the image augmentation strategy used"}
knitr::include_graphics("augmented_shrooms.png")
```

# Our CNN model architecture

Our classifier is a convolutional neural network (CNN) consising of 

- 3 convolutional layers, each followed by a pooling layer  
- A fully connected layer  
- An output layer with a single node, outputting a probability value  

The architecture of our model is visualized in the picture below. The architecture was derived by a combination of examples and recommendations and our own experimentation.

<br>

```{r, out.width = "350px", fig.cap = "Architecture of our CNN"}
knitr::include_graphics("model.png")
```

## Convolutional layers

A matrix convolution is a linear operation which transforms the input by 
"sliding" a 3 x 3 convolution matrix (in our case) over the input array and produces a new feature map. 
The convolution matrix first looks at the area in the upper left corner of size (3,3,3) and produces a single output 
by performing a matrix multiplication over that area.
It then moves one pixel to the right and does the same. It continues over the whole image.

The above operation results in a (478, 478, 1) feature map; a new representation of the input images. 
The representation depends on the values in the convolution matrix. 
These values are treated as parameters in the network and are estimated during training to produce the optimal feature map.

The depths of our three convolutional layers are 32, 64 and 64. 
This means that in the first layer, the convolution operation is performed separately 32 times by 32 different convolution 
matrices. In the second and third convolutional layers, the operation is performed 64 times. The first convolutional layer 
therefore procudes an output of (478, 478, 32) and so on.

## ReLU activation and max pooling

Each convolutional layer is followed by a rectified linear unit (ReLU). ReLU is simple nonlinear function wich outputs:  

`max(0, input)`  

The purpose of the relu activation is to introduce nonlinearity to the feature maps. 
The matrix convolution is a linear operation and most interesting aspects of images are usually nonlinear. 

Pooling is a simple way to reduce the dimensionality of the problem and help in generalisation.
We used max pooling with pool size (2,2), which halves the input in both vertical and horizontal dimensions by outputting the maximum value of two disjoint consequent values in the feature map.

## Dense layers and output

After the convolutional layer, there is a fully connected layer with 64 nodes. This means that each node uses the output of each previous node in a linear equation. Each node then uses a ReLU activation to introduce nonlinearity. 

Finally, a single output node connects to each of the previous 64 nodes. A linear equation is again followed by a nonlinear one, this time the sigmoid function, which outputs values between 0 and 1. These values can be interpret as probabilities, and they can be used for predicting the class of the image. A natural choice is to predict a mushroom as eatable, if the probability of it being eatable is at least 0.5. Tis is not the only possible choice, however, as one might wish to be more conservative to avoid eating poisonous mushrooms.

# Training the network

The image data was splitted to training and testing sets with a split of 80% to 20%. The CNN was then trained by feeding it small batches of augmented image data from the training set, 32 images at a time. Each slightly altered image was passed through the network a 100 times, i.e. the network was trained on 100 *epochs*.  

The picture below shows both the training and testing accuracy for each of the epochs. It can be seen that while the training accuracy has an increasing trend, the testing accuracy averages a steady 55% accuracy, which is remarkably poor. 

```{r, out.width = "600px", fig.cap = "Training and testing accuracy of our CNN by epoch"}
knitr::include_graphics("training_history.png")
```

It should be noted that an identical network was trained with the same strategy but possibly different training and testing data, and that network achieved higher testing accuracy (the network was trained again to save and display the training history). It is quite possible that due to the small amount of data, the training set might not include suitable examples of all eatable and non-eatable mushrooms available in dataset, which would naturally affect performance.


# Model predictions

Once the network has been trained, it can be used to make predictions. Our model outputs a single value describing the probability that a mushroom is eatable.

```{r, out.width = "600px", out.height = "400px", fig.cap = "An example of a prediction of the CNN"}
knitr::include_graphics("prediction_example.png")
```

## Precision and recall

Since our model outputs a probability for edibility, the user can make a choice to only pick mushrooms that have a very high probability of edibility. The higher that treshold is, the less likely it is that the hunter will collect poisonous or inedible mushroom (high precision). However, the tradeoff is that the hunter is also more likely to leave some edible mushrooms in the forest (low recall).

The picture below shows the precision and recall of our classifier, by the probability treshold chosen. 

```{r, out.width = "600px", fig.cap = "Precision and recall of our mushroom classifier by the probability treshold. The probabilities are given in percentages in the x axis."}
knitr::include_graphics("precision_recall.png")
```

# Building the app


# Sources

- [Intuitive explanation of CNNs](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)  
- [Sienten tunnistaminen](http://www.funga.fi/teema-aiheet/sienten-tunnistaminen/)  
- [CNN binary classification tutorial using litte data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)  
