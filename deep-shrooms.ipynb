{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepShrooms\n",
    "\n",
    "Our goal is to classify pictures of (common) mushrooms using some kind of web-app.\n",
    "\n",
    "Challenge is to get good quality data for the training and then negate some common problems with images such as: lighting, angle, blurriness and background noise.\n",
    "\n",
    "Current plan is to classify only the poisonous mushrooms of Finland along with some common edible and un-edible ones. Probably using Convolutional Neural Network.\n",
    "\n",
    "\n",
    "Preliminary Model:\n",
    "- name-fin - Finnish name\n",
    "- name-eng - English name\n",
    "- name-latin - Latin name\n",
    "- url-mw - Mushroom-world url\n",
    "- url-wiki? - Wikipedia url\n",
    "- url-lajit? - Lajit.fi url\n",
    "- type - edible/poisonous/un-edible(or neutral?)\n",
    "\n",
    "## Sources of data\n",
    "\n",
    "Mushroom World\n",
    "http://www.mushroom.world\n",
    "\n",
    "Lajit\n",
    "http://tun.fi/HBF.25786?locale=fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_labels = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIG WARNING\n",
    "\n",
    "We are here going to scrape the data from the Mushroom World -website. Getting the basic data is kinda tiny but fetching the pictures will probably be in range of hundreds of MBs. Which is why this part of code will be either commented out or moved elsewhere and the images will be saved in Google Drive and downloaded from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Scrape mushroom url\n",
    "#\n",
    "# scrape_mushroom_url takes as input an url to a page in mushroom.world containing information\n",
    "# related to a single mushroom and returns a python dictionary of information (including image url's) \n",
    "# related to the mushroom.\n",
    "#\n",
    "# @param url An url to a mushroom web page in mushroom.world\n",
    "# \n",
    "# @return \n",
    "# Returns a python dictionary containing the following keys:\n",
    "# - name1: (string) Name of the mushroom.\n",
    "# - name2 (string) Name given in parenthesis. Can be '' if no such name was given\n",
    "# - images: (list) A list of image urls\n",
    "# - info: (dict) A dictionary of information related to the mushroom. \n",
    "#       keys: Family, Location, Dimensions, Edibility, Description (dict)\n",
    "#           Description keys: General, Cap, Gills, Stem\n",
    "# \n",
    "# @examples\n",
    "# \n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# \n",
    "# url = 'http://www.mushroom.world/show?n=Galerina-marginata'\n",
    "# mushroom = scrape_mushroom(url)\n",
    "# print(mushroom)\n",
    "# \n",
    "def scrape_mushroom(url):\n",
    "    \n",
    "    # retrieve site data as BeautifullSoup object\n",
    "    data  = requests.get(url).text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "    # extract and parse name, labels (Family, Location, Dimensions, Edibility, Description)\n",
    "    # and content text related to the labels\n",
    "    name_content = soup.find(class_ = \"caption\").find(\"b\").contents\n",
    "    names = re.sub('[^A-Za-z0-9( ]+', '', name_content[0]).split(\"(\")\n",
    "    names = [n.strip() for n in names]\n",
    "    name1 = names[0]\n",
    "    if(len(names) > 1):\n",
    "        name2 = names[1]\n",
    "    else:\n",
    "        name2 = ''\n",
    "\n",
    "    labels = soup.find_all(class_ =\"labelus\")\n",
    "    labels = [label.contents[0] for label in labels]\n",
    "\n",
    "    texts = soup.find_all(class_ = \"textus\")\n",
    "    texts = [text.contents[0] for text in texts]\n",
    "\n",
    "    # extract mushroom description as a dictionary\n",
    "    description = soup.find(class_ = \"longtextus\").contents\n",
    "    description = [re.sub('[^A-Za-z0-9,.<> ]+', '', str(d)).strip() for d in description]\n",
    "    description = [re.sub('<b>', '', d) for d in description if (d != \"\") & (d != \"<br>\")]\n",
    "    description.insert(0, 'General')\n",
    "    description = dict(zip(description[0::2], description[1::2]))\n",
    "\n",
    "    texts.append(description)\n",
    "    assert len(labels) == len(texts)\n",
    "    \n",
    "    # find image urls\n",
    "    images = soup.find(id=\"mushroom-list\").find_all(class_ = \"image\")\n",
    "    image_urls = ['http://www.mushroom.world' + image.a[\"href\"] for image in images]\n",
    "\n",
    "    # contruct the mushroom dictionary\n",
    "    mushroom = dict(name1 = name1, name2 = name2, images = image_urls, info = dict(),)\n",
    "\n",
    "    # add labels as keys and text as values\n",
    "    for i in range(len(labels)):\n",
    "        mushroom[\"info\"][labels[i]] = texts[i]\n",
    "\n",
    "    return mushroom\n",
    "\n",
    "def scrape_mushrooms(df):\n",
    "    return [scrape_mushroom(url) for url in df['url-mw']]\n",
    "\n",
    "mw_scraped = scrape_mushrooms(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = 'mushroom_img'\n",
    "\n",
    "import time\n",
    "\n",
    "def download_mushroom_imgs(df, dict_scraped, path):\n",
    "    imgs = {}\n",
    "    for row in df.itertuples():\n",
    "        # uses name-latin as image's name -> Cantharellus cibarius -> cantharellus_cibarius\n",
    "        img_name = row[2].lower().replace(' ', '_')\n",
    "        img_paths = []\n",
    "        for index, img_url in enumerate(dict_scraped[row.Index]['images']):\n",
    "            img_data = requests.get(img_url).content\n",
    "            img_ext = img_url[-3:].lower()\n",
    "            full_path = os.path.join(path, \"{}{}.{}\".format(img_name, index, img_ext))\n",
    "            # download img and save it to full_path\n",
    "            with open(full_path, 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "            time.sleep(4)\n",
    "        imgs[row.Index] = img_paths\n",
    "    return imgs\n",
    "\n",
    "mw_imgs = download_mushroom_imgs(test_labels[1:2], mw_scraped, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name-fin                name-latin  \\\n",
      "1  Suppilovahvero  Cantharellus tubaeformis   \n",
      "\n",
      "                                              url-mw    type  \n",
      "1  http://www.mushroom.world/show?n=Cantharellus-...  edible  \n",
      "{'suppilovahvero', 'kangaskärpässieni', 'korvasieni', 'kavalakärpässieni', 'kitkerälahokka', 'keltavahvero', 'lampaankääpä', 'ruskokärpässieni', 'isohapero', 'peltoherkkusieni', 'suippumyrkkyseitikki', 'herkkutatti', 'männynleppärousku', 'punakärpässieni', 'valkokärpässieni', 'kuusenleppärousku', 'myrkkynääpikkä', 'haaparousku'}\n"
     ]
    }
   ],
   "source": [
    "#[row for row in test_labels.itertuples()]\n",
    "print(test_labels[1:2])\n",
    "dum = {row[1].lower().replace(' ', '_') for row in test_labels.itertuples()}\n",
    "print(dum)\n",
    "#'http://www.mushroom.world/data/fungi/Cantharelluscibarius1.JPG'[-3:].lower()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
